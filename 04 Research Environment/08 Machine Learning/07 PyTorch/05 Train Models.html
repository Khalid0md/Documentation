<p>You need to <a href="/docs/v2/research-environment/machine-learning/pytorch#04-Prepare-Data">prepare the historical data</a> for training before you train the model. If you have prepared the data, build and train the model. In this example, create a deep neural network with 2 hidden layers. Follow these steps to create the model:</p>

<ol>
    <li>Define a subclass of <code>nn.Module</code> to be the model.<br></li>
    <p>In this example, use the ReLU activation function for each layer.</p>
    <div class="section-example-container">
        <pre class="python">class NeuralNetwork(nn.Module):
    # Model Structure
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(5, 5),   # input size, output size of the layer
            nn.ReLU(),         # Relu non-linear transformation
            nn.Linear(5, 5),
            nn.ReLU(),  
            nn.Linear(5, 1),   # Output size = 1 for regression
        )
    
    # Feed-forward training/prediction
    def forward(self, x):
        x = torch.from_numpy(x).float()   # Convert to tensor in type float
        result = self.linear_relu_stack(x)
        return result</pre>
    </div>

    <li>Create an instance of the model and set its configuration to train on the GPU if it's available.</li>
    <div class="section-example-container">
        <pre class="python">device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = NeuralNetwork().to(device)</pre>
    </div>
    
    <li>Set the loss and optimization functions.</li>
    <p>In this example, use the mean squared error as the loss function and stochastic gradient descent as the optimizer.</p>
    <div class="section-example-container">
        <pre class="python">loss_fn = nn.MSELoss()
learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</pre>
    </div>

    <li>Train the model.</li>
    <p>In this example, train the model through 5 epochs.</p>
    <div class="section-example-container">
        <pre class="python">epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    
    # Since we're using SGD, we'll be using the size of data as batch number.
    for batch, (X, y) in enumerate(zip(X_train, y_train)):
        # Compute prediction and loss
        pred = model(X)
        real = torch.from_numpy(np.array(y).flatten()).float()
        loss = loss_fn(pred, real)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch
            print(f"loss: {loss:.5f}  [{current:5d}/{len(X_train):5d}]")</pre>
    </div>
    <img class="docs-image" src="https://cdn.quantconnect.com/i/tu/pytorch-train.png" alt="pytorch model training summary">
</ol>